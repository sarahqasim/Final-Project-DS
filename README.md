# Final-Project-DS

This project aims to bridge that gap by developing a computer vision-based system that translates American Sign Language (ASL) hand gestures into text. 
ASL is a common type of sign language form used in North America with different hand gestures and motions. 
The model will classify images of hand gestures and map them to corresponding letters of the alphabet.

Dataset: The Sign Language MNIST dataset was used, containing labelled images of ASL hand gestures.
A Convolutional Neural Network (CNN) was implemented using TensorFlow and Keras for feature extraction and classification.
